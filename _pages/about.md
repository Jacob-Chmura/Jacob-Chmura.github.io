---
layout: about
title: Home
permalink: /
subtitle:

profile:
  align: center
  image: prof_pic.jpg
  image_circular: true # crops the image to make it circular
  address: üìç Toronto, Canada

news: false # includes a list of news items
latest_posts: false # includes a list of the newest posts
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

<hr>

I'm currently working in [applied deep reinforcement learning at RBC](https://www.rbccm.com/en/expertise/electronic-trading/ai-trading.page). My efforts are evenly split between research and engineering. 
On the research front, I'm building predictive models to tackle the [order routing problem](https://en.wikipedia.org/wiki/Smart_order_routing) and
investigating information theoretic signals that accelerate learning in sparse reward environments. 
Previously, I developed a multi-objective reinforcement learning framework *(patent pending)*, that finds policies on the pareto front and enables few-shot adaptation to linear preferences at inference time.
On the engineering side, I'm focused on increasing training throughput, reducing inference latency, and lowering GPU memory requirements all around.

Before RBC, I was a reseacher at the [Vector Institute for Artificial Intelligence](https://vectorinstitute.ai/), applying machine learning to solve problems in cancer genomics.
Specifically, I investigated the extent to which bayesian methods, ensembles, and second order optimization could reduce rare cancer missclassification rates.
I ran a feature importance analysis, and worked on a information-theoretic algorithm that segments the genome based on regional mutation density patterns, reducing the average number of mutations required to discriminate cancer types.

My primary research interests are statistical decision theory, representation learning, and self-supervision. 
In particular, I want to build the [foundation model](https://en.wikipedia.org/wiki/Foundation_models) for control. 
In this direction, I believe [compression based objectives](https://arxiv.org/pdf/0812.4360.pdf) can give rise to generalized representations that improve sample efficiency and accelerate exploration.

I am also fascinated by the intersection of machine learning and systems, designing a runtime that supports efficient training and inference of machine learning models at scale.
Finding subquadratic attention operators that can be efficiently implemented with optimizations like kernel fusion and tiling will be critical for long-horizon decision making agents.

<hr>
